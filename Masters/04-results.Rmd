# Results {#results}
<!--
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, engine.path = list(python = '/anaconda/bin/python'))
```
-->
```{r load30minHouse, include=FALSE}
# Choose which househould we want to examine
# Available houses:
# "06","08","13","22","24","25","27","29","30","31","32",
# "33","34","35","36","37","38","40","42","44","45"

# House 30 has NAs in HW column
house <- "13"

load(paste0("~/HWCanalysis/Masters/data_old/house_",
            house, "_at_30.Rda"))

if (!exists("dFile")){
  dFile <- "~/HWCanalysis/Masters/data/" 
}

load(paste0(dFile, "DT_hh.Rda"))

```

```{r loadSummaryStats}

DFsummary <- readRDS(paste0(dFile, "allModelSummaryStats.rds"))
dfSummary <- readRDS(paste0(dFile, "allHouseModelStats.rds"))
```
This chapter uses a selection of models fitted to our half hour averaged data in order to forecast the demand of the following half hour.

While a more sophisticated model that provides a binary on/off output will be considered in Chapter \@ref(one-min), this project hopes to "build up" the sophistication of the modelling from more mathematically simple techniques.
In our case, this may be obtained through taking the average hot water electricity demand over each half-hour within the data. This smooths the demand data such that it may be closer to approximating to be a continuous variable. 
```{r hhElementPlot, out.width='100%', fig.cap="Half-hour averaged hot water electricity use over one day"}
knitr::include_graphics(paste0(pFile, "oneDay30min.pdf"))
```
Figure \@ref(fig:hhElementPlot) may be compared with Figure \@ref(fig:elementPlot) in Chapter \@ref(methodology) to see the smooting effect of half-hour averaging.

## Naive model

```{r loadRWdata}
# Should be unnecessary now
#load(paste0(dFile, "models/randomWalk/procTime.Rda"))
#load(paste0(dFile, "models/randomWalk/sdResRW.Rda"))
```

In our naive model we assume no drift, and only attempt to predict one (half hour) time step onto the future. From Equation \@ref(eq:randomWalk), we get 
\begin{equation}
\hat x_{t+1} = x_t .
\end{equation}

```{r randomWalk, out.width='100%', fig.cap="Actual electricity use and naive model approximation over one day for four households"}
knitr::include_graphics(paste0(pFile, "randomWalk/fourHouses.pdf"))
```

As can be seen in Figure \@ref(fig:randomWalk), this method is reasonably effective for some houses, in particular those that have long periods where the element power has roughly the same output. In contrast, households that exhibit frequent oscillation of element power are very poorly modelled by this method, with the predicted values consistantly 'missing' the actual values.

The random walk model has RMSE ranging between between `min(dfSummary$RMSE[dfSummary$model == "naive"])`W and `max(dfSummary$RMSE[dfSummary$model == "naive"])`W, with the average being `DFsummary$RMSE[DFsummary$model == "naive"]`W.

The model precisely mimics the actual data with a half-hour lag, and therefore scores highly for physical fidelity. Interpretability is also high, as the output can be understood by the simple equation $\hat x_{t+1} = x_t$. The computational time to fit models to each household varied between `min(dfSummary$fittingTime[dfSummary$model == "naive"])`s and `max(dfSummary$fittingTime[dfSummary$model == "naive"])`s, with the average taking `DFsummary$fittingTime[DFsummary$model == "naive"]`s.

## Linear regression of hot water electricity against lagged other electricity

While simple linear regression is a useful method of analysing data that has a roughly linear trend, it is generally inadequate when constructed using time series data that displays cyclic behaviour, such as residential electricity demand.
Where it may be useful, however, is in predicting the relationship between hot water electricity demand and non hot water electricity demand.
The significance of the cross-correlogram (Fig. \@ref(fig:crossCovariance)) in the initial positive lags suggests we may be able to construct a simple linear model which forecasts hot water electricity use based on the previous half-hour's non hot water electricity use.

```{r linearRelations, out.width='100%', fig.cap="Linear relation between non hot water electricity and half-hour lagged hot water electricity for household rf06"}
knitr::include_graphics(paste0(pFile, "rf06LinearPlot.pdf"))
```

Figure \@ref(fig:linearRelations) shows a simple linear regression of current non hot water electricity demand with the following half-hour's hot water electricity demand. This is shown to demonstrate the ability to use current electricity demand to predict hot water electricity demand over the next half hour. Note the low $R^2$ value suggesting that, in it's current form, this model would be rather inadequate at forecasting demand.

```{r simpleLinear, out.width='100%', fig.cap="Actual electricity use and simple linear model approximation using previous half hour's non-hot water electricity over one day for four households"}
knitr::include_graphics(paste0(pFile, "simpleLinear/fourHouses.pdf"))
```
Figure \@ref(fig:simpleLinear) shows the effectiveness of this model for four seperate households. Note that the top two households in the figure, `rf_13` and `rf_30`, show the model fitting reasonable poorly with the data. 
Household `rf_13` appears to predict increase in hot water electricity demand before it actually occurs. 
This should be compared to Figure \@ref(fig:crossCovariance), which shows the correllation between hot water and not hot water electricity peaking at around 200 minutes lag for this household.
Household `rf_30` shows a model which appears to be very unresponsive to the data. Again, on close examination of Figure \@ref(fig:crossCovariance) we see that this household has a peak in crosscorrelation at around 800 minutes. 
This indicates that for some households, a correlation lag of greater than 30 mins may increase accuracy. 
Households `rf_42` and `rf_44` show a more accurate model result, however due to the presence of a non-zero y intercept these models will always consistantly overestimate hot water electricity whenever the true value is zero.

The simple linear regression model has RMSE ranging between between `r min(dfSummary$RMSE[dfSummary$model == "simpleLinear"])`W and `r max(dfSummary$RMSE[dfSummary$model == "simpleLinear"])`W, with the average being `r DFsummary$RMSE[DFsummary$model == "simpleLinear"]`W.

The model does not accurately capture zero values, and is significantly smoother than the actual data, however no negative values are present. It therefore scores low to moderate for physical fidelity. 
Interpretability is high, as the output can be understood by the simple equation $\hat x_{t+1} = \alpha_0 + \alpha_1 w_t$, where $w_t$ is non hot water electricity at time $t$.  The computational time to fit models to each household varied between `r min(dfSummary$fittingTime[dfSummary$model == "simpleLinear"])`s and `r max(dfSummary$fittingTime[dfSummary$model == "simpleLinear"])`s, with the average taking `r DFsummary$fittingTime[DFsummary$model == "simpleLinear"]`s.

Notably, this model has lower RMSE's than the Naive model.
In addition, this model lends itself to being utilised as an input into more complex models that take other parameters into consideration. 

_Consider moving much of this optimal lag time part to the previous chapter_
We can also check to see whether half-hour in advance is the optimum length of time in advance to make predictions from. Figure \@ref(fig:crossCovariance) indicates that for many houses, hot water electricity is correlated with non hot water electricity at longer timescales, as can be seen by the locations of the local maximums. 

Table \@ref(tab:ccvMaxLagTable) and the corresponding Figure \@ref(fig:ccvMaxLagPlot) have been constructed by taking the maximum cross covarience value for each house, restricted to lag values above 30 minutes, as this is the minimum lag after which these predictions may be useful for participation in demand response markets in New Zealand.

```{r loadCCVdata, echo=FALSE}
load(paste0(dFile, "ccv_max.Rda"))
maxCor <- maxCor[, c(2,1,3)]
```

```{r ccvMaxLagTable, echo=FALSE}
kable(maxCor,
      col.names =  c("Household", "Lag", "Cross covariance"))
```


While Table \@ref(tab:ccvMaxLagTable) and Figure \@ref(fig:ccvMaxLagPlot) show that many houses have their maximum values at the boundary (30 minutes), around half have their maximum cross-correlations occurring at longer timescales.

```{r ccvMaxLagPlot, out.width='100%', fig.cap="Lag at maximum correllation for each household (bin width 30 minutes)"}
knitr::include_graphics(paste0(pFile, "maxXcorAllHouses.pdf"))
```

## Moving average

_I might as well include a moving average model. It is easy to impliment and would be a good segue into ARIMA_ 

## ARIMA model

Some[@Cui2016] forecasting of electricity hot water use has been done using autoregressive moving average (ARIMA) models.
_include plots and results_
_rerun results with a max.p = 50 to facilitate seasonal differencing_

```{r ARIMA, eval = FALSE}
AA <- auto.arima(dt$HWelec, max.p = 50)

HWarima <- arima(dt$HWelec, seasonal = time3, method = "CSS")

plot(AA)

```
_Show parameters of autoarima_

## Vector Autoregression

_This needs moving to methodology_
We now explore forecasts using a vector autoregression model. 
The model is of the following form:
\begin{equation}
\bf{y}_t = A_1 \bf{y}_{t-1} + … + A_p \bf{y}_{t-p} + CD_t + \bf{u}_t ,
\end{equation}
where $\bf{y}_t$ is a $K \times 1$ vector of endogenous variables and $u_t$ assigns a spherical disturbance term of the same dimension. The coefficient matrices $A_1, …, A_p$ are of dimension $K \times K$. 

Our model is fitted using the function VAR within the package vars, which estimates the vector autoregression model using ordinary least squares. 

The model is constructed using inputs of hot water demand and non hot water demand, along with the integers of time steps that correspond to any cyclical effects (referred to as 'seasonality'), and the number of steps by which to lag the model by.

For our data, we utilise the cyclical values previously determined using frequency analysis as the "seasonal" input.
 
The optimal lag term can be determined using the VARselect function, which optimises lag values by minimising the Bayesian information criterion using the formula

\begin{equation}
SC(n) = \ln \det(\tilde{\sum}_u(n)) + \frac{\ln(T)}{T} ,
\end{equation}

where $\tilde{\sum}_u (n) = T^{-1} \sum_{t=1}^T \bf{\hat{u}}_t \bf{\hat{u}}_t'$ and n assigns the lag order.

A separate analysis on our data was carried out using this function, with the maximum lag to consider set to 200 for computational punctuality. While the optimal lag varied somewhat for each household, a fixed value to input for each household was desired in order to simplify analysis that involved multiple houses. The value was ultimately fixed at 50 as this was the median optimal $SC(n)$ lag for the houses considered. _Note this seems to be a larger value than traditionally used_

```{r VARModelling, include=FALSE, eval = FALSE }
#install.packages("remotes")
#remotes::install_github("DavZim/varsExtra")
library(varsExtra) # refer to previous two lines for installation

# Note that when built using 1 minute data, this model is only useful
# for prediction 1 minute into the future. 

# This may be overcome using the forecast() function, however it may not be
# physically accurate as I don't believe smart meters currently log with this frequency
dt2 <- dt[ , 2:3]
var1 <- vars::VAR(dt2, p = 50, season = 24, type = "none") # Create model with cyclical effects determined previously using frequency analysis

#p <- ggfy(var1) # Plot adjusted below
#p
#ggsave("sdexample.png")

# NOTE removing the season input from the model often makes it slightly more accurate.
# this is very strange, investigate further!
```


```{r ggfyPrep,  include=FALSE, eval = FALSE}
vals <- var1$datamat %>%
  dplyr::as_tibble() %>%
  dplyr::select(-dplyr::matches("\\.l[0-9]+")) %>%
  dplyr::mutate(type = "data", t = 1:n()) %>%
  tidyr::gather(key = "var", value = "value", -type, -t)

ypred <- lapply(1:length(var1$varresult), function(i) {
  dplyr::tibble(value = as.numeric(fitted(var1$varresult[[i]]))) %>%
    dplyr::mutate(type = "fitted",
                  t = 1:n(),
                  var = names(var1$varresult)[i])
}) %>% dplyr::bind_rows()

resid <- lapply(1:length(var1$varresult), function(i) {
  dplyr::tibble(value = as.numeric(resid(var1$varresult[[i]]))) %>%
    dplyr::mutate(type = "residuals",
                  t = 1:n(),
                  var = names(var1$varresult)[i])
}) %>% dplyr::bind_rows()

vardf <- dplyr::bind_rows(vals, ypred, resid) %>%
  dplyr::select(var, type, t, value) %>%
  dplyr::arrange(var, type)

# NOTE: dplyr::filter method of selection was dropping every second variable
df3 <- vardf[vardf$var == "HWelec", ]
df3 <- df3[df3$type != "residuals", ]

tv <- as.POSIXct(dt$datetime)
timeV <- tail(tv,length(tv) - 50)

dfSimp <- cbind(df3, timeV)

dfSimp$datehour <- cut(dfSimp$timeV, breaks="hour") 
dfSimp$datehour <- gsub(".* ", "", as.character(dfSimp$datehour))

```

```{r zoomedIn,eval = FALSE,  include=FALSE, fig.cap="Behaviour of data and VAR model over 48 hours"}

# select bounds of t to "zoom in" on a section of the plot
p <- dfSimp %>%
dplyr::filter(t >=24, t < 48*2+24) %>%
ggplot(aes(x = t, y = value, color = type,
                             linetype = type)) + geom_line()
p+labs(x = "Time", y = "Power (W)")
 scale_x_datetime(labels = time_format("%H:%M"))

# for reference 

#p <- ggplot(data <- avDT, aes(x = hour, y = value, color = variable,
#                              linetype = variable)) +
#  geom_line() 
#p + labs(x = "Hour of day", y = "Average power (W)", colour = "", linetype = "") + 
#  scale_x_datetime(labels = date_format_tz())

#time_format("%H:%M")
```

```{r VARACF,eval = FALSE,  include=FALSE, fig.cap="Autocovariance of VAR model"}

dfFitted <- vardf %>%
dplyr::filter(type == "fitted" & var == "HWelec")


p <- ggAcf(x = dfFitted$value, lag.max = weekCycle/2, type = "correlation",
    plot = TRUE, na.action = na.pass)
p + labs(x = "Lag (half hours)", y = "Autocovariance", title = "")

```

```{r hourlyAverages,eval = FALSE, include=FALSE, fig.cap="Hourly averages of hot water use"}

library(data.table)

dfSimp <- data.table(dfSimp)
avData <- dfSimp[type=="data",list(avg=mean(value)),by=datehour]
colnames(avData) <- c("hour", "data")
avModel <-dfSimp[type=="fitted",list(avg=mean(value)),by=datehour]
colnames(avModel) <- c("hour", "model")

avDT <- melt(as.data.table(cbind(avData, avModel$model)))
avDT$hour <- as.POSIXct(avDT$hour, format= "%H")

avDT <- avDT[order(avDT$hour),,drop=FALSE]
levels(avDT$variable) <- c('data', 'fitted')

avDT$variable <- as.character(avDT$variable)
#avDT$variable <- as.factor(avDT$variable)

# work-around to fix time display
# may not be necessary once my comp time is fixed

# Source: http://stackoverflow.com/a/11002253/496488
# Put in your local timezone
#date_format_tz <- function(format = "%H:%M", tz = "UTC") {
#  function(x) format(x, format, tz=tz)
#}

#labels = date_format_tz

# Need to fix 

p <- ggplot(data <- avDT, aes(x = hour, y = value, color = variable,
                              linetype = variable)) +
  geom_line() 
p + labs(x = "Hour of day", y = "Average power (W)", colour = "", linetype = "") + 
  scale_x_datetime(labels = time_format("%H:%M"))

```

```{r frequencyPlot, eval = FALSE, include=FALSE, fig.cap="Frequency of power observations"}
p <- ggplot(dfSimp, aes(x = value, fill = type)) +
  geom_histogram(binwidth = 100) +
  facet_wrap(. ~ type)
p + labs(y = "Observations",
       x = "Power (W)") + # use colour-blind friendly palette
  theme(legend.position = "bottom")
```

Figure \@ref(fig:frequencyPlot) shows the frequency of observations of power (100W bin width) for both the data and the model.
It can be seen that the model does not fit the data accurately in this regard. In part this may be due to regular occurrences of the element only needing to reheat the water slightly due to thermal losses from the wall of the tank, or very minor hot water usage events.

```{r residualDF, eval = FALSE}
#residuals(var1)

HWres <- var1$varresult$HWelec$residuals
HWresAbs <- abs(HWres)
meanHWres <- mean(HWres)
meanHWresAbs <- mean(HWresAbs)

sdRes <- sd(HWres) 
sdResAbs <- sd(HWresAbs)
```

```{r residualACF, eval = FALSE, fig.cap="Autocovariance of the VAR model residual"}
p <- forecast::ggAcf(HWres, lag.max = weekCycle*2, type = "correlation",
    plot = TRUE, na.action = na.pass)
p + labs(x = "Lag", y = "Autocorrelation", title = "")
```

This plot of the autocorrelation function of the residuals (Fig. \@ref(fig:residualACF) shows that autocorrelation of residuals is negligible, however some cyclical behaviour can be observed. In particular, many houses display a weekly peak (at lag `r weekCycle`). This is perhaps due to the weekly cycle not being taken into consideration with all houses.

The absolute value of the residuals is calculated, and the mean of this is taken to demonstrate the error of this model for the individual house selected.

```{r residualsPlot, eval = FALSE}
#hidden for now - doesn't really add anything
plot(HWres, type = "c")
```

<!--
The mean value and standard deviation of the residual is r round(meanHWres, 1) W and  r round(sdRes, 1)  respectively, whereas the mean and standard deviation of the absolute value of the residual is  r round(meanHWresAbs, 1) W and  r round(sdResAbs, 1)  respectively. This suggests that while the VAR model residual at any one time for an individual house may be too large for accurate forecasting, the addition of more houses may significantly reduce this.
-->
_NOTE: work out error as percent of element power?_

To explore this further we examine a dataframe consisting of the residuals of the various houses (see `scripts/VAR_residual_df.R`). 

```{r loadResidualDF, eval = FALSE}
load(file = "~/HWCanalysis/Masters/data/resDF.Rda")
load(file = "~/HWCanalysis/Masters/data/amalgamatedRes.Rda")
```

The mean residual when all r length(unique(resDF$house)) houses are considered together is r round(mean(w[,"HWres"]), 1)W, whereas the mean of the absolute value of all houses considered together is r round(mean(w[, "HWresAbs"]), 1)W.

```{r cyclicalMatrix, eval = FALSE}
#incomplete - an attempt to determine the component of the VAR model responsible for cyclical effects
#cycle <- var1$varresult$HWelec$qr$qr
#plot(cycle)
#acf(cycle)
```
The VAR model provides a reasonably accurate means of forecasting. It is be possible to forecast half an hour in advance using minute data but this may not be practically implimentable in reality (smart meters may not store minute data as such).

## Generalised Additive Models with continuous response variable
