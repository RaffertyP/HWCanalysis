# Methodology {#methodology}

This chapter begins by giving an introduction to the dataset that was used for this research. It goes on to describe some necessary preliminary cleaning of the data. Mathematical descriptions of the methods and models utilised in the main body of work are then introduced, with applicability to the context of hot water electricity demand forecasting provided. Finally, the metrics by which the models are compared with one another are outlined.

All data processing and modelling was conducted using the `R` programming language[@R-base].
In particular, data extraction and processing used the packages `GREENGridData`[@R-GREENGridData], `dplyr`[@R-dplyr] and `data.table`[@R-data.table]. Time series manipulation and analysis used packages `lubridate`[@R-lubridate], `forecast`[@R-forecast], and `xts`[@R-xts]. Plots were created using the packages `ggplot2`[@R-ggplot2], `ggplotmisc`[@R-ggpmisc], and `gridExtra`[@R-gridExtra]. Tables were created using `knitr`[@R-knitr] and `kableExtra`[@R-kableExtra]. Additional functions and packages used to create the models are mentioned after the description of the corresponding model.
Significant consideration has been given to documenting each step of the analysis process and facilitating reproducibility of results. This will assist in furthering the research undertaken.

<!-- Required to number equations in HTML files 
-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r loadDTandpc_rm, include=FALSE}
# This loads percentage of data removed
load(paste0(dFolder, "pc_rm"))
DThead <- readRDS(paste0(dFolder, "DThead.rds"))
```

## Data background

The data used for this thesis was collected from monitored submetered electrical power usage of 44 households in Hawkes Bay and Taranaki, New Zealand, at one minute intervals between 2014 and 2018 as part of the GREEN Grid project[@GREENGrid]. 

The dataset is publicly available from the UK Data Service. Publications to date that have utilised the dataset include [@Ocampo2015], [@Suomalainen2017], [@Stephenson2018], [@JackKiti2018], and [@JackDew2018].

Individual households have labels beginning with `rf_`, followed by unique identifying numbers. Note that these numbers are non-sequential. 
This labelling format is maintained thoughout this research in order to simplify cross-referencing with other research that uses this dataset.

More information about this dataset, including detailed reports of data issues and access instructions, is available at https://cfsotago.github.io/GREENGridData/. 

## Preliminary data analysis

In any data analysis, there are preliminary processes that should be undertaken in order to get a general overview of the data[@Cowpertwait2009]. This allows for an informed opinion to be made as to the best techniques to achieve the desired objectives of the analysis.
This section explores our dataset for the purpose of ascertaining any patterns and attributes that may assist in our predictive modelling. It starts by explaining the cleaning and preparation process, then uses methods to compare and visualise cycles and correlations within relevant variables. The analysis draws upon a broader overview of the data available at https://cfsotago.github.io/GREENGridData/.

### Initial data cleaning

Some of the households in the original dataset are not suited to the purposes of this analysis. Three were removed immediately (`rf_15`, `rf_17` and `rf_46`) due to issues with the data collection process (see https://github.com/CfSOtago/GREENGridData/issues/21 and https://github.com/CfSOtago/GREENGridData/issues/19 for more information). 
Data files from the remaining households are unzipped and processed using the `GREENGridData` package[@R-GREENGridData]. Total electricity is imputed from the submeters using the script `imputeTotalPower.R` (obtained from the `GREENGridData` github repository). From this output, imputed total electricity demand and hot water electricity demand were extracted using `GREENGridData::extractCircuitFromCleanGridSpy1min.R`. The outputs from this script then require some further cleaning and processing to be suitable for our analysis.
During the preliminary data exploration, a number of households in the dataset were found to have characteristics that meant they were unsuitable for this analysis, and were removed. These are as follows:

Households `rf_07`, `rf_09`, `rf_10`, `rf_17b`, `rf_19`, `rf_21`, `rf_26`, `rf_28`, `rf_41`, `rf_43`, `rf_47` did not have separate hot water metering. 
Households `rf_23` and `rf_24` had hot water controlled by either a timer or a home energy management system in order to maximise self-consumption of their solar PV.
Household `rf_11` had a heat-pump hot water system, which rather than a typical on/off element, instead had constant electricity draw of around 54W.
Household `rf_17a` had extremely low hot water electricity values, indicating a problem with the sensor. 
Households `rf_27`, `rf_01`, `rf_15b`,  had periods of days, weeks, or even months where no hot water electricity was used interspersed with (somewhat) normal usage. All these households were therefore discarded from further analysis.

In addition, household `rf_31` only collected zero values for hot water electricity after 26th of February 2016.  Rather than discarding this household, it was instead cropped so as to only contain values before this date.

For the remaining households, hot water electricity demand is subtracted from total electricity demand, giving two separate columns: hot water electricity, and all other electricity. 

Many of the time series analysis packages used in this work require perfectly sequential data collection, with no missing (or '`NA`') values.
As such, any 'holes' in our data were dealt with as follows.
When long periods of missing data occurred (determined by visual inspection of preliminary plots) the largest period of uninterrupted data collection was selected for further analysis, with the remainder discarded.

```{r prelimNoHolesRemoved, out.width='100%', fig.cap="Overview of data before cleaning process"}
knitr::include_graphics(paste0(pFolder, "prelim/allHousesNoHolesRemoved.jpg"))
```

```{r prelimHolesRemoved, out.width='100%', fig.cap="Overview of data after cleaning process"}
knitr::include_graphics(paste0(pFolder, "prelim/allHousesAfterRemoval.jpg"))
```

The effect of removing these larger holes can be seen by comparing Figures \@ref(fig:prelimNoHolesRemoved) and \@ref(fig:prelimHolesRemoved).

Smaller holes in data were dealt with by inserting zero values of electricity power where necessary. Zero values were selected as opposed to using averages or other methods to reflect the 'on/off' nature of the HWC element at 1 minute timescales. This technique facilitates further analysis at this level of granularity if required.

While analysis at 1 minute timescales may be beneficial in a future where metering and transmitting infrastructure is capable of facilitating control at this detail, this research attempts to develop methods that could be implemented using existing smart meters.
Smart meters in New Zealand currently store and transmit data that has been averaged over half hour periods, thus we further process our data to imitate this by averaging electricity power over each half hour time step.
This has the effect of 'smoothing' our data, which may be seen in Figure \@ref(fig:elementPlot). 

```{r elementPlot, out.width='100%', fig.cap="Comparison of 1 minute hot water electricity data with its half-hour averaged version"}
knitr::include_graphics(paste0(pFolder, "elementComparisonPlot.png"))
```


```{r headDT}
DThead %>%
  knitr::kable(caption = "Example of the clean and processed data used in the analysis")
```

While minor additional processing was necessary for particular models, the fundamental form of the data used throughout the analysis is the form shown in Table \@ref(tab:headDT).
The `hHour` column is the timestamp, `linkID` represents the household, and `nonHWelec` and `HWelec` are the half-hour averaged electricity demand of other appliances and hot water, respectively.

Data processing code can be viewed in Appendix \@ref(code). 

```{r load_pkg, include = FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
# Load packages (thesisdown will load all of the packages as well)
library(thesisdown)
```

```{r setup1, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(GREENGridData)
library(ggplot2)
library(ggpmisc)
#library(vars)
library(dplyr)
library(TSA)
library(forecast)
library(knitr)
library(scales)
library(lubridate)
library(xts)
```

### Time series overview and notation {#TSoverview}

As much of this research is concerned with predictability and patterns within our data over time, we extensively utilise a data science technique called 'time series analysis'. This section introduces the notation and some key concepts relating to time series data and its analysis. For further information or clarification, the book "Introductory Time Series With R"[@Cowpertwait2009] provides a good overview to the process. 

A time series $\{x_1, x_2, ..., x_n\}$, also abbreviated to $\{x_t\}$, is a collection of $n$ samples of data taken at discrete times $\{t = 1,2,...,n\}$ [@Cowpertwait2009].
In this research, $\{x_t\}$ refers to the half-hour averaged hot water electricity demand.
Models may be constructed by adjusting their internal parameters in order to best fit this data. This provides the ability to predict a future value based on historical data values. In this thesis, models are denoted using the 'hat' notation, where $\{\hat x_t\}$ is the _model_ of $\{x_t\}$. <!-- A prediction at time $t$ of a value $k$ steps forward is denoted $\{\hat x_{t+k | t}\}$.-->
There are a number of terms and concepts that assist in providing clear and succinct mathematical descriptions of time series analysis methods and models.

The expected value, $\operatorname{E}$ is indispensible in time series methods. The expected value of a discrete random variable is the probability-weighted average of all its possible values. This is defined mathematically as follows.

Let $X$ be a random variable with a finite number of finite outcomes $x_1, x_2, \ldots, x_k$ occurring with probabilities $p_1, p_2, \ldots, p_k,$ respectively. Then
\begin{equation}
  \operatorname{E}[X] =\sum_{i=1}^k x_i\,p_i=x_1p_1 + x_2p_2 + \cdots + x_kp_k.
\end{equation}

Note that when $p_1 = p_2 = \dots = p_k$, the expected value is equal to the mean.

Another common statistical method used to measure the amout of variation within data is the standard deviation, $\sigma$. For a collection of $T$ measurements of time series data, $\sigma$ is given by

\begin{equation}
  \sigma = \sqrt{\frac{1}{T-1}\sum_{i=1}^T (x_i - \bar{x})^2 },
\end{equation}

Another useful concept is that of covariance. Covariance (denoted $\operatorname{Cov}$) is a measure that is used when we have multivariable time series data (such as that comprised of both hot water electricity demand and other appliance demand). It is defined as the expected value (or mean) of the product of their deviations from their individual expected values:
\begin{equation}
  \operatorname{Cov}(X,Y) = \operatorname{E}\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]
\end{equation}

Also commonly used to describe time series analysis methods is the backshift operator.
The backshift operator $\textbf{B}$ shifts the value it operates on to the previous value in the series, i.e. $\textbf{B}x_t = x_{t-1}$.
This may be raised to arbitrary powers to shift values further in time, i.e.

\begin{equation}
  B^k x_{t} = x_{t-k}.
  (\#eq:BSO)
\end{equation}

The difference between an observed value and the value predicted by a model, $\hat x_t - x_t$, is formally known as the residual. This can be intuitively understood as the error in the value predicted by the model. 
The residual sum of squares ($RSS$) is the sum of the squares of all the residuals over the times considered:

\begin{equation}
  \operatorname{RSS} = \sum_{i=1}^n (x_i - \hat x_t)^2
\end{equation}
Many models considered in this research are fitted algorithmically in order to minimise the RSS subject to parameter constraints.

Models that effectively capture the underlying properties of their data have residuals that approximate _white noise_.
A white noise time series, $\{w_t : t = 1,2,...,n\}$ is a set of independent and identically distributed variables with zero mean.
This implies that $\operatorname{E}\{w_j\} = 0$ for all $j$, and that 

\begin{equation}
  \operatorname{Cov}(w_k, w_j) = 
   \begin{cases}
      \sigma^2,& \text{if } k = j\\
      0,&\text{if } k \ne j
    \end{cases}
\end{equation}

where $\sigma$ is the standard deviation.


### Autocovariance

To fit with known properties of total residential electricity demand[@Jack2018], we would expect residential hot water electricity demand to fluctuate with daily and weekly periodicity due to the routines of the household occupants. This periodicity is often observed in annual timescales in econometric and financial time series data, such as an increase in house sales during summer months[@Wooldridge2009]. As many time series methods were developed to analyse econometric and financial data, these periodic fluctuations are referred to as 'seasonality'.

There are a number of different methods which allow us to explore seasonality within data. One method is autocovariance. Autocovariance compares the covariance of a stochastic process (such as our time series data of hot water electricity use) with itself at different time steps. This is a valuable tool for visualising and quantifying cyclical behaviour of data, and is defined as

\begin{equation}
 \gamma_k = \operatorname{E}\big[(x_t - \mu) (x_{t+k} - \mu)\big]
  (\#eq:ACV),
\end{equation}

where $k$ is the lag value, $\operatorname{E}$ is the expected value operator, and $\mu$ is the mean of both $x_t$ and $x_{t+k}$.
<!--
![Autocovariance of all houses.](~/HWCanalysis/Masters/plots/acf_all_houses.png)
-->

The results of the autocovariance function are interpreted by plotting $\gamma_k$ against the lag values. This was obtained using the `Acf` function from the `forecast` package[@R-forecast].

### Fourier analysis

While autocovariance plots are valuable for preliminary data analysis, models that incorporate these cycles need more precise numerical values of these cycles. One method of obtaining usable cycle periods is through Fourier analysis.
Fourier analysis offers us the ability to automate the process of determining cycles in our data. 
It achieves this by representing a set of time series data in terms of periodic functions[@Bloomfield2000].
While a description of the precise mechanics of how this is achieved is beyond the scope of this thesis, the general idea is as follows.

```{r sineDemonstration, out.width="100%", fig.cap="Demonstration of the properties of sinusoids"}
knitr::include_graphics(paste0(pFolder, "sinExample.pdf"))
```

A sinosoid, in the context of time series, is a periodic function given by the equation

\begin{equation}
  y = Rsin(ft + \phi),
\end{equation}
where $R$ is the amplitude of the sinusoid, $f$ is its frequency, $\phi$ is its phase, and t is time.
Varying the amplitude increases the 'height' of the curve, varying the phase 'shifts' the curve horizontally, varying the frequency increases the frequency of peaks and troughs. These curves may be added to one another to form curves that demonstrate a combination of the properties of those they are composed of.
An example is provided in Figure \@ref(fig:sineDemonstration).

Fourier analysis attempts to replicate the data as a linear combination of sine curves and similar peroodic functions, optimising the parameters $R$, $f$ and $\phi$ in order to minimise the square of the residuals. Once the optimised composite function is fitted to the data, values for amplitude, frequencies and phases may be extracted. The dominant frequencies are those which correspond to the sinosoids with the highest amplitudes[@Brockwell1991].
Using the function `periodogram` from the `TSA` package we can extract the most dominant cycle frequencies within our data by Fourier analysis. 
These may then be input into later models that have the option of accounting for seasonality. 

### Cross covariance with other appliance electricity demand

```{r bothElecPlot, echo=FALSE, out.width='100%', fig.cap="Electricity demand of hot water and other appliances over one day"}
knitr::include_graphics(paste0(pFolder, "bothElecPlot.pdf"))
```
Many households appeared to show instances whereby the electricity demand of other appliances increases before hot water electricity demand. This may be understood by considering general domestic behavioural patterns. Many people may choose, for example, to turn on the kettle before a shower in the morning, or cook dinner before doing the dishes in the evening. An example demonstrating this behaviour is given in Figure \@ref(fig:bothElecPlot).
We can make a more thorough exploration into the temporal relationship between hot water electricity demand and that of other appliances through examinaing their cross-covariance.

In a similar fashion to the autocovariance function (Equation \@ref(eq:ACV)), the cross-covariance function of two variables $x, y$ is given by

\begin{equation}
  \gamma_k(x,y) = \operatorname{E}\big[(x_{t+k} - \mu_x)(y_t - \mu_y)\big] ,
\end{equation}
where variable $x$ lags variable $y$ by lag $k$[@Cowpertwait2009].

Plotting the cross-covarience (known as a _cross correlogram_) allows visual inspection of the relation between the two variables at different time lags. 
This is done using the `Ccf` function from the `forecast` package[@R-forecast].

### Seasonal and trend decomposition using Loess{#STL}

Another method that may be used to explore any cyclical effects or trends of time series data is that of STL decomposition. STL is an acronym for 'seasonal and trend decomposition using Loess', whereby Loess (short for 'local polynomial regression') is a method for estimating nonlinear relationships. 
An STL decomposition separates the data into three components, referred to as the 'trend', 'seasonality', and 'remainder' (also referred to as 'random'), which added to one another make up the original data. 

The trend represents low frequency changes in the data, along with longer term average shifts. 
Seasonality refers to the cyclic behaviour of the data. The remainder is the deviation of the actual data from the addition of the trend and the seasonality.
The STL method was developed by Cleveland, Cleveland, McRae, & Terpenning (1990)[@Cleveland1990]. As the decomposition algorithm is quite involved it is not included here, but can be examined in the previous reference. For our analysis, STL decomposition was conducted using the R packages `stats`[@R-base] and `forecast`[@R-forecast].

```{r STLPlot, out.width='100%', fig.cap="Seasonal decomposition of two weeks of data from household 35"}
knitr::include_graphics(paste0(pFolder, "rf_35_STL.pdf"))
```
Figure \@ref(fig:STLPlot) illustrates an STL decomposition of a household over two weeks. The top panel, labelled 'observed', is the actual data, while the lower three panels display the trend, seasonal, and random components of this data.

## Model selection

Every forecasting model has unique strengths and weaknesses that render it more or less suitable to different applications.
In the context of forecasting residential hot water electricity demand, there are a number of considerations that have led to some models being selected for further investigation, while others are passed over without consideration.
This work attempts to make predictions based only on data that would be available from any smart meter with a separately metered hot water cylinder, as is commonly available in New Zealand today. For this reason, causal models that rely on the availability of external data are not investigated further.
As this work is concerned mainly with making predictions only half an hour into the future, time series methods that attempt to uncover longer-term trends in averages are also not relevant.
Throughout this process I have placed strong emphasis on allowing reproducibility. 
For this reason, any forecasting methods that required high capacity computational resources in order to successfully execute were not considered. 

## Introduction to selected models {#IntroToModels}

This section outlines a broad introduction to the various models selected for use either implicitly or explicitly in this analysis.
Note that white noise terms, $\{w_t\}$ are included within model descriptions.
This represent the residual of the model, as discussed in Section \@ref(TSoverview).
As white noise time series values are inherently unpredictable with an expected value of zero, these terms are always equal to zero when models are used for demand forecasting.
If the same model was used for demand simulations however, the white noise term would instead be artificially generated[Lomet2015].

For further details regarding SVM or ANN models, refer to [@Magoules2016].
For further details regarding ARIMAX models, refer to [@DeFelice2013].
Further details regarding all other models mentioned in this chapter can be found in [@Cowpertwait2009]. 

### Naive model {#naive}

In keeping with best practice[@Gelazanskas2015], a very simplistic model is selected by which to compare the performance of more complicated models to.
The model selected for this task is known as a 'random walk'.

A random walk takes the form 

\begin{equation}
  \hat x_t = x_{t-1} + w_t ,
  (\#eq:randomWalk)
\end{equation}
where $\{w_t\}$ is a white noise series. 

Random walk models are more commonly used in simulations than forecasts. 
When used in simulations, the white noise is artificially generated to simulate a potential evolution of the variable in time.
When used in forecasting, as always within this research, the white noise term is allocated its expected value of zero. Consequently, the value of the dependant variable $x$, (hot water electricity demand, in our case) of the next timestep can be best predicted by its value in the current timestep ($\hat x_{t+1} = x_{t}$).
Due to their simplicity and ease of computation they are sometimes used in forecasting in this context as a 'benchmark' model, by which other models may be compared. For this reason, they are often referred to as 'naive' models.

The naive models used in this research were created using the `naive` function within the `forecast` package[@R-forecast].

### Seasonal naive model

In a similar manner to the naive model, the seasonal naive model estimates the next value in a series from a single prior observation.
However, the seasonal naive model makes the assumption that for periodic data, the most likely value of the the next timestep is that of the same time one period prior.
For data that display weekly seasonality, for example that of residential electricity demand, a seasonal naive model would thus predict that hot water demand is the same as it was the same time one week prior. For data at half-hour resolution, one week before the current time step is $48 \times 7 = 336$ time steps prior, thus

\begin{equation}
  \hat x_{t} = x_{t-336} + w_{t}.
  (\#eq:snaive)
\end{equation}

Seasonal naive models were created using the `snaive` function from the `forecast` package[@R-forecast].

### Simple linear regression

Linear modelling is a common forecasting method prized for its simplicity in interpretation and computation.
A special case of a linear model is the simple linear regression, which fits a straight line through data in order to minimise the RSS. 
For time-series forecasting, simple linear regression can fit a line to historical values of the dependent (or 'response') variable in order to estimate future values.
This line is fitted to minimise the RSS, and is given by $x_t = \gamma_0 + \gamma_1 t$.
While this can be useful for determining general trends over longer periods of time, they cannot capture any regular shorter term fluctuations of time series data about this trend (seasonality). 

Another way of utilising simple linear regression for time series forecasting is in the form of a causal model by introducing a separate predictor variable or variables. For our data, we separate electricity demand into hot water, and other appliances. Simple linear regression is a foundational method of exploring how the demand of other appliances (denoted $y$) at time $t$ can be used to predict hot water electricity demand at a time $t + 1$.

This is given by the relation

\begin{equation}
  \hat x_{t} = \gamma_0 + \gamma_1 y_{t-1} + w_{t}.
(\#eq:simpleLinearRegression)
\end{equation}

Our simple linear regression models were constructed using the `lm` function from the `stats` package[@R-base].

### Differencing {#diff}

Differencing a time series $\{x_t\}$ is a simple method of removing trends and cycles in order to approximate time series data as white noise. 
This is often used in combination with other methods which become more more accurate when applied to a white noise series.

Differencing is often referred to as an integrated model $I^d$, and is defined using the backshift operator as

\begin{equation}
  I^d = (1-\textbf{B})^d,
\end{equation}

where d is an integer.
A time series $\{x_t\}$ is referred to as _integrated_ of order $d$ if the $d$th difference of $\{x_t\}$ is white noise.
This is demonstrated in Figure \@ref(fig:integratedExample), whereby total household electricity data more closely resembles white noise after differencing.

```{r integratedExample, fig.cap="Total electricity demand data before and after differencing"}
knitr::include_graphics(paste0(pFolder, "differencingExample.pdf"))
```

Note that the naive model in Section \@ref(naive) is the special case $I^1$.

### Autoregression

An autoregression model of order $p$, referred to as $AR(p)$, can be represented as
\begin{equation}
  \hat x_t = \alpha_1x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t ,
  (\#eq:AR)
\end{equation}

where $\{w_t\}$ is white noise and $\alpha_i$ are model parameters, and $\alpha_p \neq 0$.

This can be represented using the backshift operator as

\begin{equation}
  \hat x_t = \sum_{i=1}^p \alpha_i \textbf{B}^i x_t + w_t,
\end{equation}

and by moving the summation to the left, this may be expressed even more succinctly in polynomial notation as

\begin{equation}
  \theta_p(\textbf{B}) \hat x_t = (1 - \alpha_1\textbf{B} - \alpha_2\textbf{B}^2 - ... - \alpha_p\textbf{B}^p)\hat x_t = w_t,
\end{equation}

where $\theta_p$ is a polynomial of order $p$. 

### Moving average

A q-order moving average model, $MA(q)$ (where $q$ is an integer) can be expressed as a linear combination of the white noise residual $w_t$ (see Section \@ref(TSoverview)) and the $q$ most recent previous residuals, defined as

\begin{equation}
  \hat x_{t} = \operatorname{E}[x_t] + w_{t} + \beta_1 w_{t-1} + ... + \beta_q w_{t-q} .
  (\#eq:MA1)
\end{equation}

If $\operatorname{E}[x_t] = 0$, which may be artificially induced by prior processing with the differencing method in the Section \@ref(diff), Equation \@ref(eq:MA1) may be expressed as

\begin{equation}
  \hat x_t = (1 + \beta_1 \textbf{B} + \beta_2 \textbf{B}^2 + ... + \beta_q \textbf{B}^q)w_t = \phi_q\textbf{B}w_t ,
\end{equation}

where $\textbf{B}$ is the backshift operator (see Equation \@ref(eq:BSO)), and $\phi_q$ is a polynomial of order $q$.


### ARIMA {#ARIMAmethodology}

An Autoregressive Moving Average (ARMA) process of order $(p,q)$ combines autoregression with the moving average process, adding the two together. This results in 

\begin{equation}
  x_t = \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t + \beta_1 w_{t-1} + \beta_2 w_{t-2} + ... + \beta_q w_{t-q} .
  (\#eq:ARMA)
\end{equation}
This may be expressed in terms of the backward shift operator in polynomial form as
\begin{equation}
  \theta_p (\textbf{B}) x_t = \phi_q (\textbf{B}) w_t .
  (\#eq:ARMApolyform)
\end{equation}

The previously described processes are frequently combined into what as known as an autoregressive integrated moving average (ARIMA).
If data is differenced $d$ times in an integrated model $I^d$ before an $ARMA(p,q)$ model is fitted, the output is referred to as an $ARIMA(p,d,q)$ model.

Autoregression, integrated, and moving average models may then be considered individually as special cases of ARIMA models; $ARIMA(p,0,0)$, $ARIMA(0,d,0)$ and $ARIMA(0,0,q)$ respectively.
Values for $p,d,q$ are selected in order to best fit the data while minimising computational expense. Larger values of $p,q,d$ tend to increase accuracy, while taking longer to compute.
The process of selecting optimal values for $p,d,q$ can be automated through minimising the Akaike Information Criterion (AIC)[@Akaike1974], where

\begin{equation}
  AIC = -2\times \text{log-likelihood} + 2\times \text{number of parameters}.
  (\#eq:AIC)
\end{equation}

The R function `auto.arima`[@R-forecast] was used to automatically select the parameters $p,d,q$ which minimise the AIC specific to the particular data being modelled. This is done iteratively according to the algorithm outlined in ref [@RobJ.Hyndman2008]. Inputs for maximum values of $p$ and $q$ are necessary in order to bound processing time. 

## Seasonal ARIMA

In a similar manner to how trends can be removed through differencing at lag 1, seasonal effects within data (such as daily use patterns) can be removed by differencing at lag $s$, where $s$ is the length of the season.
A seasonal ARIMA model may also introduce additional autoregressive and moving average terms at lag $s$, giving a model of the form $ARIMA(p,d,q)(P,D,Q)_s$. This may be expressed in polynomial notation as

\begin{equation}
  \Theta_P(\textbf{B}^s)\theta_p(\textbf{B})(1-\textbf{B}^s)^D(1-\textbf{B})^d x_t = \Phi_Q(\textbf{B}^s)\phi_q(\textbf{B})w_t .
\end{equation}

### STL with ARIMA

An alternative mechanism by which to incorporate cyclic effects into ARIMA models is through applying STL decomposition (see Section \@ref(STL)) before model fitting. A time series may be split into seasonal, trend, and remainder components, with the remainder component being modelled as an ARIMA process in the same manner as described in Section \@ref(ARIMAmethodology). The seasonal and trend components are then added back to the ARIMA modelled remainder as the complete predictive model.
This method was the most accurate model considered in [@Gelazanskas2015].
Our STL + ARIMA models were fitted using the `stlm` function from the `forecast` package[@R-forecast].

### ARIMAX {#ARIMAX}

An ARIMAX model is an ARIMA model with the addition of an external regressor. 
One way in which this may be interpreted is as a simple linear regression model with ARIMA errors.
This combines Equations \@ref(eq:simpleLinearRegression) and \@ref(eq:ARMA) in the form

\begin{equation}
  \hat x_t = \gamma_1y_{t-1} + \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + \beta_1 w_{t-1} + \beta_2 w_{t-2} + ... + \beta_q w_{t-q} + w_t .
  (\#eq:ARIMAX)
\end{equation}

Where $y$ is the external regressor.
This may be expressed in terms of the backward shift operator in polynomial form as

\begin{equation}
  \hat x_t = \frac{\gamma_1}{\phi (\textbf{B})}y_{t-1} + \frac{\theta (\textbf{B})}{\phi (\textbf{B})} w_t .
(\#eq:ARMAXpolyform)
\end{equation}

In the context of this research, the regressor $y$ is the electricity demand of other appliances.
ARIMAX models were created using the `auto.arima` function from the `forecast` package[@R-forecast].

### STL and ARIMAX

The predictive power of seasonal decomposition and external regressors may be combined with an ARIMA model, to get a model we refer to as STL + ARIMAX.
This method decomposes the data into seasonal, trend and remainder components, (refer to Section \@ref(STL)), and then fits an ARIMAX model (Equation \@ref(eq:ARIMAX)) to the remainder component using lagged values of other appliance electricity demand as external regressors (refer to Section \@ref(ARIMAX)). This is then added back to the seasonal and trend components to complete the model.
STL + ARIMAX models were created using the `stlm` function from the `forecast` package[@R-forecast].

### Artificial Neural Networks

ANNs attempt to mimic biological learning mechanisms by processing a set of training data through clusters of artificial neurons, each of which individually receive (numerical) inputs, and assign them certain weights. These weighted values are then outputted into other artificial neurons, which perform the same task, until eventually producing a final output.
The weights assigned by each neuron are then iteratively adjusted in order to minimise the error of the final output.
While ANNs are commonly used in electricity demand forecasting, literature suggests they tend to be outperformed in both accuracy and computational efficiency by support vector machines for this type of analysis[@Ahmad2014]. 
For this reason, they were not included in this comparative work, with support vector machines instead selected as the comparative AI method.

### Support vector machines

<!--
$\hat x_t = w0+w1x1+w2x2+w3x3$
-->

Support vector machines classify data points by mapping them in hyperspace and distinguishing between them according to which side of a hyperplane they fall on. The position of the classifying hyperplanes are constructed iteratively in order to maximise the perpendicular distance between each hyperplane and the closest samples on either side of it.
Figure \@ref(fig:SVMexample) provides a linear example of this. 

(ref:svm) Example demonstrating the classifying mechanism of a SVM[@Larham]

```{r SVMexample, out.width='100%', fig.cap='(ref:svm)'}
knitr::include_graphics(paste0(pFolder, "SVMmargin.png"))
```

Non-linear partitions can also be constructed using a kernel function $K$. 

The process of training a non-linear SVM can be stated mathematically as follows:
Given instances $\mathbf{x}_i ,  i= 1, . . . , l$ with labels $y_i \in \{1,-1\}$, solve the following quadratic optimization problem:
\begin{equation*}
  \begin{aligned}
    \underset{\mathbf{\alpha}}{\text{min}} \quad & f(\mathbf{\alpha}) =\frac{1}{2}\mathbf{\alpha}^TQ\mathbf{\alpha}-\mathbf{e}^T\mathbf{\alpha} \\
    \text{subject to} \quad & 0\leq \mathbf{\alpha}_i\leq C, i= 1, . . . , l, \\
    & \mathbf{y}^T\mathbf{\alpha}= 0,
  \end{aligned}
\end{equation*}
where $\textbf{e}$ is the vector of all ones, $C$ is the upper bound of all variables, $Q$ is an $l$ by $l$ symmetric matrix with $Q_{ij} = y_i y_jK(\mathbf{x}_i,\mathbf{x}_j)$, and $K(\textbf{x}_i,\textbf{x}_j)$ is the kernel function[@Fan2005].

Support vector machines were created using the `svm` function from the `e1071` package[@e1071]. This function uses the training algorithms detailed in Fan, Chen and Lin(2005)[@Fan2005].

## Training and validating

_Talk about the separation of data into training and validating here._

## Comparative metrics

There are a number of different considerations that must be made when comparing models for the process of electricity demand forecasting. This section outlines those considered relevant to this work.

### Accuracy

Perhaps the most important consideration in forecasting is the accuracy of the model. For this work, model accuracy is determined by the root mean square of the residuals, with lower values indicating higher accuracy. This is referred to as the 'root mean square error (RMSE). 
The RMSE of predicted values $\hat x_t$, where actual values are $x_t$ and predictions are observed over T time periods, is given by:

\begin{equation}
  \operatorname{RMSE}=\sqrt{\frac{\sum_{t=1}^T (\hat x_t - x_t)^2}{T}}.
  (\#eq:RMSE)
\end{equation}

For each model considered within this research, the average RMSE is taken over all households to provide the overall model RMSE.

As demand response is most crucial during daily peak periods, additional analysis is carried out to ascertain the accuracy during grid peaks (defined as 7am to 9am, and 5pm to 8pm[@TranspowerNZ2015]). This was obtained by calculating the RMSE for all predictions that occurred during peak periods (denoted $RMSE_{peak}$). To assist comparisons, this is then used to calculate a percentage error increase (PEI) between the average RMSE and the average RMSE during peak times. The PEI is given by:

\begin{equation}
  \operatorname{PEI} = \frac{RMSE_{peak} - RMSE}{RMSE} .
\end{equation}

### Physical fidelity

In addition to accuracy of prediction values, there are benefits to models that closely resemble the physical process they are predicting.  While a model that provides occasional values that are negative or greater than the element can output may provide a minimal RSS, they would not be optimal to use when modelling for research purposes. 
Physical fidelity is a measure of how closely the model approximates the physical process of the element of the hot water cylinder turning on and off in response to the draw down of hot water. Properties of decent physical fidelity include the non-existence of negative values or values above the maximum power of the element being modelled, as well as replication of the general shape of data determined by the on/off nature of the element.

### Interpretability

Another important consideration for research purposes is interpretation of results. Model interpretability is a measure of how well we can infer fundamental behavioural properties (cycles, trends, correlations) from the model. Models that are easy to interpret are valuable for understanding the human behaviour behind hot water use, as well as for building simulations for further research.
For a model to score highly in this metric, its parameters need to be easily obtainable, and preferably composed of succinct equations. 
"Black box" models, and those that output very large arrays score poorly.

### Computational efficiency

In order to make the real-time predictions necessary to effectively participate in demand response markets, models for thousands, or even hundreds of thousands of households need to be updated regularly, with reasonable speed. 
For this reason, consideration is given to the computational efficiency of each model.
This is defined as the amount of time taken to fit the model, with all models fitted sequentially using the same machine.
In addition to processing expense, the amount of storage space each model requires is also worth consideration when considering large numbers of households.
